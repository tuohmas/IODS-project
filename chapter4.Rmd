# Exercise 4, Week 4: Clustering and classification

```{r setup, include=FALSE} 
knitr::opts_chunk$set(warning = FALSE, message = FALSE) 
```

This week's exercise is performed on dataset on Housing Values in Suburbs of Boston (Boston), available with MASS-package.
[Full variable descriptions can be found here](https://stat.ethz.ch/R-manual/R-devel/library/MASS/html/Boston.html) or by calling ?MASS:Boston.

_Source:_
Harrison, D. and Rubinfeld, D.L. (1978) Hedonic prices and the demand for clean air. _J. Environ. Economics and Management_ **5**, 81â€“102.
Belsley D.A., Kuh, E. and Welsch, R.E. (1980) _Regression Diagnostics. Identifying Influential Data and Sources of Collinearity_. New York: Wiley. 

First, we will load Boston from MASS

```{r}
library(MASS)
data("Boston")

str(Boston)
summary(Boston)
dim(Boston)

```
Dataset consists of 506 observations, and 14 variables.

Before continuing to further, let's get a handle about correlations between our variables.

```{r}
library(corrplot)
library(tidyverse)

# plot matrix of the variables
pairs(Boston)

# calculate the correlation matrix and round it
cor_matrix<-cor(Boston) %>% round(digits = 2)

# print the correlation matrix
cor_matrix

# visualize the correlation matrix
corrplot(cor_matrix, method="circle", type="upper", cl.pos="b", tl.pos="d", tl.cex = 0.6)

```

Here, correlation (Pearson's _r_) between variables is shown as an matrix and as a plot. We can observe highest correlations between

* **property tax rate (tax)** and **access to radial highways (rad)** ( _r_ = 0.91);
* **mean distances to Boston employment centres (dis)** and **pollution from nitrogen oxides (nox)** ( _r_ = 0.77); and
* **nitrogen oxides concentration (nox)** and **proportion of non-retail business acres (indus)** ( _r_ = 0.76).

Specifically, **per capita crime rate by town (crim)** has

* moderate and positive relationship with access to radial highways ( _r_ = 0.63) and property tax rate ( _r_ = 0.58); and
* weak and negative relationship with **proportion of black populace** (black, _r_ = -0.39) and **median house values** (medv, _r_ = -0.39).

### Linear discrimination analysis (LDA)

Linear discriminant analysis produces results based on the assumptions that:

* variables are normally distributed (on condition of the classes)
* the normal distributions for each class share the same covariance matrix

Because of the assumptions, we need to standardize and scale our data before fitting the model. This can be done using R scale function, assigning scaled dataset to a new **boston_scaled** object.

```{r}
# center and standardize variables, and change the matrix to a data.frame
boston_scaled <- scale(Boston) %>% as.data.frame()

# summaries of the scaled variables
summary(boston_scaled)
```
Basically, we have substraced the column means from the corresponding columns and divided the difference with standard deviation. Accordingly:

* all of our variables are centered around mean, which itself has been standardized to be zero; and
* values ranging between the mimimum and the median are negative (minimum being the lowest observation below zero), while values spanning from median to maximum are positive (maximum being the largest positive observation).

Let's now focus to Boston crime rate. For the purposes of LDA we need transform interval variable **crim** into a categorial variable **crime**. One way to go about this is to use crim quantiles as cut-off points (roughly equal sized bins) with the quantile function. Afterwards, we better substitute the old variable with the new one.

```{r}
# create a quantile vector of crim and print it
bins <- quantile(boston_scaled$crim)
bins

# create a categorical variable 'crime'; label break points from low to high:
crime <- cut(boston_scaled$crim, breaks = bins, labels = 
               c("low", "med_low", "med_high", "high"), include.lowest = TRUE)

# look at the table of the new factor crime
table(crime)

# remove original crim from the dataset and add in the new categorical one:
boston_scaled <- select(boston_scaled, -crim)
boston_scaled
boston_scaled <- data.frame(boston_scaled, crime)
```

Next, we will split the data set randomly to two sets, train and test, so that train consist of 80% randomly chosen observations, and test of the remaining data set. From train, we will also substract our categorical crime variable, 

```{r}
# assign 80% of Boston rows randomly to ind 
ind <- sample(nrow(boston_scaled),  size = nrow(boston_scaled) * 0.8)

# create "train" set that consists of the sample selection ind 
train <- boston_scaled[ind,]
dim(train)

# and a "test" set out of the remaining 20 % of rows, and remove crime variable:
test <- boston_scaled[-ind,]
dim(test)

# save the correct classes from test data
correct_classes <- test$crime

# remove the crime variable from test data
test <- dplyr::select(test, -crime)
dim(test)
```
Train consists of 404 observations, and test of 102 observations and 13 variables.

Now we are ready to fit the LDA on the train set using lda() function. In our formula, we have crime (a multiclass variable) as our target, and all other variables as predictors. We will visualize our model in 2-dimensional bi-plot.

```{r}
# linear discriminant analysis
lda.fit <- lda(crime ~ ., data = train)
str(train)

# print the lda.fit object
lda.fit

# the function for lda biplot arrows
lda.arrows <- function(x, myscale = 1, arrow_heads = 0.1, color = "red", tex = 0.75, choices = c(1,2)){
  heads <- coef(x)
  arrows(x0 = 0, y0 = 0, 
         x1 = myscale * heads[,choices[1]], 
         y1 = myscale * heads[,choices[2]], col=color, length = arrow_heads)
  text(myscale * heads[,choices], labels = row.names(heads), 
       cex = tex, col=color, pos=3)
}

# target classes as numeric
classes <- as.numeric(train$crime)

# plot the lda results
plot(lda.fit, dimen = 2, col = classes, pch = classes)

lda.arrows(lda.fit, myscale = 1)

```
**(NB! Every time document is knitted as html, test and train data sets are randomly sampled anew. This will alter the model and the prediction below, so that my interpretations may be wildly off. Sorry for the inconvenience)**

From above we can see that

* Separations achieved by the first discriminant function (LD1) is 95.8%, second is 3.1%, and the third 1.1%.
* Coefficients of linear discriminants: it seems that rad variable has been instrumental in constructing the LDA decision rule for linear discriminant 1, while other variables contribute minimally. 
* When it comes to LD2, rad is once again dominant, followed by zn (proportion of residential land zoned for lots over 25,000 sq.ft.), and finally nox (nitrogen oxides concentration), that is an somewhat odd predictor.
* If we would want to make one discrimination still, nox and zn contribute the most. However, we have little to gain (1.1%) by introducing one more dimension.
* Above interpretation shines through visually as well, with most of our predictors clumped in the middle where both first and second linear discriminant are zero, the only notable exception being rad variable that correlated moderately strongly with the first dimension.
* Biplot shows that high crime rate is mostly separated from the other three categories with some overlap with medium high. Furthermore, the second dimension discriminates adequately between low and medium high categories.

### Predict LDA

We can use predict() function with our test data set to see how accurately our model can predict our observations.

```{r}
# predict classes with test data
lda.pred <- predict(lda.fit, newdata = test)

# cross tabulate the results
corr_v_pred <- table(correct = correct_classes, predicted = lda.pred$class)
corr_v_pred

# display frequencies as row precentages 
prop.table(corr_v_pred, 1) * 100

```
Cross tabulating correct classes versus predicted ones, we see that 

* Prediction of high crime rate corresponds our data completely. 
* Prediction power drops dramatically when moving to medium high crime (58.6% accurate), and diverges between three categories. 
* Classification is quite accurate (70.8%) when it comes to medium low crime rate; and 
* When it comes to medium low category, there are more false positives (51.9%) than accurate predictions.

### Distance measures and k-clustering

Let's calculate Euclidean and Manhattan distance matrices between the observation in reloaded and rescaled Boston dataset.

```{r}
# load MASS and Boston
library(MASS)
data('Boston')
boston_scaled <- scale(Boston) %>% as.data.frame()

# euclidean distance matrix
dist_eu <- dist(Boston)

# look at the summary of the distances
summary(dist_eu)

# manhattan distance matrix
dist_man <- dist(Boston, method = "manhattan")

# look at the summary of the distances
summary(dist_man)

```

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 3)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```
Above, we have randomly disriminated our dataset into three clusters, with no clear idea wheather or not this choice is optimal. To find such number, we will calculate total within sum of squares (TWCSS); in other words, search for a partition of our observations in boston_scaled into _k_ groups that minimizes the within-group sum of squares (WGSS) over all variables:

```{r}
# determine the number of clusters
k_max <- 5

# calculate the total within sum of squares
twcss <- sapply(1:k_max, function(k){kmeans(boston_scaled, k)$tot.withinss})

# visualize the results
qplot(x = 1:k_max, y = twcss, geom = 'line')

```

The line chart plots how TWCSS/WGSS behaves as number of clusters (in the x axis) changes. the optimal number of cluster is on the point Where TWCSS drops dramatically, which in our case is 2 (from over 7000 to little over 4500; more than next three drops combined)

Finally, using K-means clustering with two clusters, let's plot variables pairs, colored by clusters for our scaled data set: 

```{r}
# k-means clustering
km <-kmeans(boston_scaled, centers = 2)

# plot the Boston dataset with clusters
pairs(boston_scaled, col = km$cluster)
```
